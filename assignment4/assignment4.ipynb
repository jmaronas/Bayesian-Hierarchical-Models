{
 "cells": [
  {
   "cell_type": "raw",
   "id": "748e70fa",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"BDA - Assignment 4\"\n",
    "author: Juan Maroñas Molano\n",
    "output: \n",
    "  pdf_document: \n",
    "    toc: yes\n",
    "    toc_depth: 1\n",
    "urlcolor: blue\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3525b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(aaltobda)\n",
    "data(\"bioassay_posterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bf358",
   "metadata": {},
   "source": [
    "# Exercise 1)\n",
    "\n",
    "## a)\n",
    "\n",
    "The relation between the correlation and the covariance of a pair of random variable $X,Y$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{cov}(X,Y) = \\text{corr}(X,Y)\\sigma_X\\sigma_Y\n",
    "$$\n",
    "\n",
    "with $\\sigma$ being the standard deviation. The correlation does not influence the mean. Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803d339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mean vector\"\n",
      "[1]  0 10\n",
      "[1] \"Covariance matrix\"\n",
      "     [,1] [,2]\n",
      "[1,]  100   12\n",
      "[2,]   12    4\n"
     ]
    }
   ],
   "source": [
    "mean   = c(0,10)\n",
    "cov_xy = 0.6 * 10 * 2\n",
    "\n",
    "cov    = array(c( 10**2, cov_xy , cov_xy, 2**2 ), dim = c(2,2) )\n",
    "\n",
    "print(\"Mean vector\")\n",
    "print(mean)\n",
    "print(\"Covariance matrix\")\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73969d8",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "#### Theory: Monte Carlo Standard Error\n",
    "\n",
    "The Monte Carlo Standard Error measures the standard deviation of different estimates of a quantity of interest when these estimates are obtained from simulations based on random numbers is used. For example if we are computing the mean of a function $f(\\cdot)$ over a random variable $x$ then a possible estimator with $S$ samples is given by:\n",
    "\n",
    "$$\n",
    " \\mathbb{E}[f(x)] = \\int f(x) p(x) \\text{d}x \\approx \\frac{1}{S} \\sum_s f(x^{(s)}) = \\hat{\\mu}^{(S)}\n",
    "$$\n",
    "\n",
    "Different random generation will yield a different set of $S$ samples. Thus there will be some variance $\\frac{\\sigma^2}{S}$ associated with the estimation of $\\hat{\\mu}^{(S)}$, where $\\sigma^2$ is the variance of $p(x)$ . The square root of this variance is the Monte Carlo Standard Error i.e. $\\frac{\\sigma}{\\sqrt{S}}$. Thus, it is the error associated with the estimation due to random generation. This error goes to zero with a rate of $\\frac{1}{\\sqrt{S}}$.\n",
    "\n",
    "Also, from the Central Limit Theorem we know that the distribution of the estimator $\\hat{\\mu}^{(S)}$ will be a Gaussian distributed with mean $\\mathbb{E}[f(x)]$  and variance $\\frac{\\sigma^2}{S}$. From where we also reason that when $S \\rightarrow \\infty$ the distribution concentrates around the real expected value. \n",
    "\n",
    "Thus if a quantity of interest has a MCSE of $0.04$ then this means that the $95.40$% of Monte Carlo estimations will lie between the true mean $\\pm 2 \\times 0.04 = \\pm 0.08$. This is because $95$% of probability mass in a Gaussian distribution lies between twice the standard deviation. More formally if MCSE$=a$ then $P(|\\hat{\\mu}^{(S)}| \\leq \\mathbb{E}[f(x)] + 2 a) \\leq 0.954$.  \n",
    "\n",
    "Thus, for the above example any decimal number in the second possition is very likely ( $95$% chance ) random noise due to different set of $S$ samples. In practice $\\sigma^2$ is replaced with its empirical estimate as well. So to show the decimal possitions we will show those relevant for the $95$% interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26247bbc",
   "metadata": {},
   "source": [
    "#### MCSE for quantiles\n",
    "\n",
    "The MCSE for quantiles is computed as follows. Suppose we wish to compute the $90$% quantile, i.e. find $a$ and $b$ such that $P( a < x < b )=0.9$. For some distributions this is known analytically. If not we resort to sampling.\n",
    "\n",
    "Take a random sample $\\{x_s\\}^S_{s=1}, x_s \\sim p(x)$. Now numerically find the values of $a$ and $b$ (for example using ```numpy.quantile```). Now this quantiles are estimated as follows. Mathematically if $I$ denotes the indicator function:\n",
    "\n",
    "$$\n",
    "\\frac{1}{S}\\sum_s I(x_s \\leq a) + I(x_s \\geq b) \\approx 0.1\n",
    "$$\n",
    "\n",
    "We can thus interpret this estimation of the quantile as a Bernoulli distribution with $p=0.1$. With probability $p$ we have an outcome $1$ representing a sample $x_s$ lying in the quantile and $0$ lying outside. In particular we have the following probability distribution:\n",
    "\n",
    "$$\n",
    "p^{y}(1-p)^{1-y}\n",
    "$$\n",
    "\n",
    "Since the variance is $p(1-p)$ and we have $S$ samples then the MCSE is estimated as the variance times the square root of the number of samples as always:\n",
    "\n",
    "$$\n",
    "\\text{MCSE}_{\\text{quantile}} = \\sqrt{\\frac{p(1-p)}{S}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a3ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Alpha mcse mean\"\n",
      "[1] 0.02964869\n",
      "[1] \"Beta mcse mean\"\n",
      "[1] 0.1512003\n"
     ]
    }
   ],
   "source": [
    "## Monte Carlo Standard Error\n",
    "\n",
    "# expected value\n",
    "tot_samples     = length( bioassay_posterior$alpha )\n",
    "mcse_mean_alpha = sd( bioassay_posterior$alpha ) / sqrt( tot_samples ) * 2 # multiplied by 2 see theory above\n",
    "mcse_mean_beta  = sd( bioassay_posterior$beta  ) / sqrt( tot_samples ) * 2\n",
    "\n",
    "print(\"Alpha mcse mean\")\n",
    "print(mcse_mean_alpha)\n",
    "\n",
    "print(\"Beta mcse mean\")\n",
    "print(mcse_mean_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dd1328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Alpha mcse 5% quantile\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.0520082319500234"
      ],
      "text/latex": [
       "0.0520082319500234"
      ],
      "text/markdown": [
       "0.0520082319500234"
      ],
      "text/plain": [
       "[1] 0.05200823"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Alpha mcse 95% quantile\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.0841268335837158"
      ],
      "text/latex": [
       "0.0841268335837158"
      ],
      "text/markdown": [
       "0.0841268335837158"
      ],
      "text/plain": [
       "[1] 0.08412683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Beta mcse 5% quantile\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.140862501837043"
      ],
      "text/latex": [
       "0.140862501837043"
      ],
      "text/markdown": [
       "0.140862501837043"
      ],
      "text/plain": [
       "[1] 0.1408625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Beta mcse 95% quantile\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.482425775414747"
      ],
      "text/latex": [
       "0.482425775414747"
      ],
      "text/markdown": [
       "0.482425775414747"
      ],
      "text/plain": [
       "[1] 0.4824258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5% quantile alpha\n",
    "print(\"Alpha mcse 5% quantile\")\n",
    "mcse_quantile(bioassay_posterior$alpha, prob = c( 0.05 ))$mcse * 2 # multiplied by 2 see theory above\n",
    "\n",
    "# 95% quantile alpha\n",
    "print(\"Alpha mcse 95% quantile\")\n",
    "mcse_quantile(bioassay_posterior$alpha, prob = c( 0.95 ))$mcse * 2\n",
    "\n",
    "# 5% quantile beta\n",
    "print(\"Beta mcse 5% quantile\")\n",
    "mcse_quantile(bioassay_posterior$beta, prob = c( 0.05 ))$mcse * 2\n",
    "\n",
    "# 95% quantile beta\n",
    "print(\"Beta mcse 95% quantile\")\n",
    "mcse_quantile(bioassay_posterior$beta, prob = c( 0.95 ))$mcse * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62214c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean of alpha parameter  1.0'</span>"
      ],
      "text/latex": [
       "'Mean of alpha parameter  1.0'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean of alpha parameter  1.0'</span>"
      ],
      "text/plain": [
       "[1] \"Mean of alpha parameter  1.0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean of beta parameter  11.'</span>"
      ],
      "text/latex": [
       "'Mean of beta parameter  11.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean of beta parameter  11.'</span>"
      ],
      "text/plain": [
       "[1] \"Mean of beta parameter  11.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Mean\n",
    "col_mean = colMeans( bioassay_posterior, dims = 1 )\n",
    "\n",
    "alpha_mean = col_mean['alpha']\n",
    "beta_mean  = col_mean['beta']\n",
    "\n",
    "sprintf( 'Mean of alpha parameter ', alpha_mean, fmt = '%s %#.1f' )\n",
    "sprintf( 'Mean of beta parameter ', beta_mean , fmt = '%s %#.0f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc49a37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'5% interval of  alpha parameter  -0.5'</span>"
      ],
      "text/latex": [
       "'5\\% interval of  alpha parameter  -0.5'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'5% interval of  alpha parameter  -0.5'</span>"
      ],
      "text/plain": [
       "[1] \"5% interval of  alpha parameter  -0.5\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'95% interval of alpha parameter  2.6'</span>"
      ],
      "text/latex": [
       "'95\\% interval of alpha parameter  2.6'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'95% interval of alpha parameter  2.6'</span>"
      ],
      "text/plain": [
       "[1] \"95% interval of alpha parameter  2.6\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'5% interval of  beta parameter  4.'</span>"
      ],
      "text/latex": [
       "'5\\% interval of  beta parameter  4.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'5% interval of  beta parameter  4.'</span>"
      ],
      "text/plain": [
       "[1] \"5% interval of  beta parameter  4.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'95% interval of beta parameter  19.'</span>"
      ],
      "text/latex": [
       "'95\\% interval of beta parameter  19.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'95% interval of beta parameter  19.'</span>"
      ],
      "text/plain": [
       "[1] \"95% interval of beta parameter  19.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 90% Quantiles of marginal distributions\n",
    "\n",
    "# alpha\n",
    "quantile_alpha = quantile(bioassay_posterior$alpha, probs = c( 0.05, 0.95 ))\n",
    "\n",
    "q5_alpha  = quantile_alpha[\"5%\"]\n",
    "q95_alpha = quantile_alpha[\"95%\"]\n",
    "\n",
    "sprintf( '5% interval of  alpha parameter ', q5_alpha , fmt = '%s %#.1f' )\n",
    "sprintf( '95% interval of alpha parameter ', q95_alpha, fmt = '%s %#.1f' )\n",
    "\n",
    "# beta\n",
    "quantile_beta  = quantile(bioassay_posterior$beta, probs = c( 0.05, 0.95 ))\n",
    "\n",
    "q5_beta  = quantile_beta[\"5%\"]\n",
    "q95_beta = quantile_beta[\"95%\"]\n",
    "\n",
    "sprintf( '5% interval of  beta parameter ', q5_beta , fmt = '%s %#.0f' )\n",
    "sprintf( '95% interval of beta parameter ', q95_beta, fmt = '%s %#.0f' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eea38",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "\n",
    "What the book refers to as importance sampling is more commonly known (or at least that is my feeling when reading) as self-normalized importance sampling. I'll first to through importance sampling and then through self-normalized importance sampling.\n",
    "\n",
    "#### Importance sampling\n",
    "\n",
    "Suppose we want to estimate the marginal distribution $p(x)$ comming from integrating out  $p(x \\mid a)$ against $p(a)$. We also have a proposal distribution $q(a)$ which is a distribution from where obtaining samples is easy.  Suppose that these distributions are properly normalized. Then using standard probability rules we have:\n",
    "\n",
    "$$\n",
    "%\n",
    "p(x) = \\int p(x| a ) p(a) \\text{d} a = \\int p(x| a ) \\frac{p(a)}{q(a)}q(a) \\text{d} a \\approx \\\\\n",
    "%\n",
    "\\frac{1}{S}\\sum_s p(x \\mid a^{(s)} ) \\frac{p(a^{(s)})}{q(a^{(s)})}; a^{(s)} \\sim q(a) \n",
    "$$\n",
    "\n",
    "Where $w^{(s)} = \\frac{p(a^{(s)})}{q(a^{(s)})}$ denote the importance weights. \n",
    "\n",
    "Note that, ideally, all the importance weights should be $1$ for optimal estimation. If the importance weights are very high or very low (eventhough the variability is small) then we won't be correctly estimating the integral. This can happen if the support of both the target and the proposal densities are very disjoints. If this is the case the samples used to estimate the integral will correspond to regions of low density from the target distribution. The weights in this case will be very small. The problem is that the Effective sample size in this case can be near to optimal (see below) unless we have the chance to sample a weight where $p(a)$ is high. So if this is the case it will be useful to plot an histogram of the weights. If they are small we know what is happening.\n",
    "\n",
    "Another problem is the one explained in the book page 265. I guess the problem here is that the ESS will be very small since the small set of big weights will tend to dominate the importance sampler estimation. This set of small weights will correspond to the tails of the target distributions and so we will be loosing estimation in the bulk since the contribution of the weights will be small relative to those in the tails.\n",
    "\n",
    "#### Self - normalized importance sampling.\n",
    "\n",
    "Self-normalized importante sampling employs a similar idea when the target distribution is known up to a normalization constant. Without loss of generality let:\n",
    "\n",
    "$$\n",
    "p(a) = c\\hat{p}(a)\\\\\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "%\n",
    "p(x) = \\int p(x \\mid a ) p(a) \\text{d} a = \\int p(x \\mid a ) \\frac{c \\hat{p}(a)}{q(a)}q(a) \\text{d} a = \\\\\n",
    "%\n",
    "%\n",
    "c  \\int p(x \\mid a ) \\frac{\\hat{p}(a)}{q(a)}q(a) \\text{d}a\n",
    "%\n",
    "$$\n",
    "\n",
    "Thus we need something that cancels out $c$. This is achieved by introducing the following integral in the denominator:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{c  \\int p(x \\mid a ) \\frac{\\hat{p}(a)}{q(a)}q(a) \\text{d}a}{ \\int p(a) \\text{d}a} = \\\\\n",
    "%\n",
    "%\n",
    " \\frac{c  \\int p(x \\mid a ) \\frac{\\hat{p}(a)}{q(a)}q(a) \\text{d}a}{ c \\int \\hat{p}(a) \\text{d}a} = \\\\\n",
    " %\n",
    " %\n",
    " \\frac{ \\int p(x \\mid a ) \\frac{\\hat{p}(a)}{q(a)}q(a) \\text{d}a}{ \\int \\frac{\\hat{p}(a)}{q(a)}q(a) \\text{d}a} \\approx \\\\\n",
    " %\n",
    " %\n",
    " \\frac{\\sum_s p(x \\mid a^{(s)} ) \\frac{p(a^{(s)})}{q(a^{(s)})}}{\\sum_s\\frac{p(a^{(s)})}{q(a^{(s)})}}; a^{(s)} \\sim q(a) \n",
    "$$\n",
    "\n",
    "yielding the usual formula of the self-normalized importance sampler:\n",
    "\n",
    "$$\n",
    "%\n",
    "%\n",
    "\\frac{\\sum_s p(x \\mid a^{(s)} ) w^{(s)} }{\\sum_s w^{(s)}}\n",
    "%\n",
    "%\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82da188",
   "metadata": {},
   "source": [
    "#### Effective sample size\n",
    "\n",
    "Note that, as mentioned above, the importance sampling approximation performance depends on the choice of $q(a)$. If $q(a)$ is not good (for example if we sample in regions of low density $p(a)$), the estimator will not be good. Also if we have a weight which is much bigger relative to other weights, it will tend to dominate the estimation. Thus, the effective sample size measures how many out of the $S$ samples are somehow relevant for the estimation. Ideally we would like $N_{\\text{eff}}=S$. For example if one weight dominates over all then ideally only one out of $S$ samples is being relevant in practice, so $N_{\\text{eff}}=1$ in that case. \n",
    "\n",
    "Ideally, we would like the variance of our importance sampler estimator to be equal to that of independent Monte Carlo, i.e. $\\frac{\\sigma}{\\sqrt{S}}$. The variance of the importance sampler is given by:\n",
    "\n",
    "$$\n",
    "\\sigma^2 \\frac{(\\sum_s w^{(s)})^2}{\\sum_s w^{(s)^2}}\n",
    "$$\n",
    "\n",
    "If we equal it to $\\frac{\\sigma}{\\sqrt{S}}$ and solve for $S$ yields:\n",
    "\n",
    "$$\n",
    "S = \\frac{(\\sum_s w^{(s)})^2}{\\sum_s w^{(s)^2}}\n",
    "$$\n",
    "\n",
    "If the weights are normalized i.e. $\\sum_s w^{(s)} = 1$ then:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{\\sum_s w^{(s)^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "yielding the result from the book. This is just a brief explanation since the theory underlying this is bigger. Also there are other ways to get to this solution of the effective sample size, and more advanced techniques here, see e.g. https://arxiv.org/abs/1507.02646. Note that very big weights will make $S_\\text{eff}$ small. If the weights are very very small then $S_\\text{eff} > 1$ something that does not make sense. This is an indicator of the sampler possibly missing parts of the target distribution with high density (for example heavy tails).\n",
    "\n",
    "\n",
    "As recommended by the book, ploting the histogram of the weights is also helpfull to analyze if the importance distribution is working correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf5f49",
   "metadata": {},
   "source": [
    "#### Make log computations\n",
    "\n",
    "Why should we compute log ratios instead of ratios is to avoid numerical underflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f7427",
   "metadata": {},
   "source": [
    "#### Exercise set up\n",
    "\n",
    "The exercise asks to compute the log importance ratios. We have the following unormalized target distribution (which is the posterior). It corresponds to the bioassay problem, which is modelled using a generalized linear model. In this case the likelihood is a binomial distribution where its parameter is computed by a linear transformation of the input $x_i$ associated to the label $y_i$. The relation between the parameters we want to infer and the parameter of the binomial likelihood is given by:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\text{logit}^{-1}(\\alpha + \\beta x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "Logit is just a link function (in this case the sigmoid) which ensures that $\\theta_i $lies in the range $[0,1]$ as required by the Binomial likelihood. Thus the likelihood is:\n",
    "\n",
    "$$\n",
    "p(y_i\\mid\\alpha,\\beta,n_i,x_i) \\propto (\\text{logit}^{-1}(\\alpha + \\beta x_i))^{y_i}(1-\\text{logit}^{-1}(\\alpha + \\beta x_i))^{n_i-y_i}\n",
    "$$\n",
    "\n",
    "And the unormalized posterior is:\n",
    "\n",
    "$$\n",
    "p_u = \\prod_i p(y_i\\mid\\alpha,\\beta,n_i,x_i)  p(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "where $p(\\alpha,\\beta)$ is a Bivariate Gaussian distribution. The $\\prod_i$ comes from assuming observations are i.i.d. This means that the weights of our importance sampler will be given by:\n",
    "\n",
    "$$\n",
    "w_s = \\frac{ p(y_i\\mid\\alpha_s,\\beta_s,n_i,x_i)  p(\\alpha_s,\\beta_s)}{p(\\alpha_s,\\beta_s)}; \\alpha_s,\\beta_s \\sim p(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "since the prior $p(\\alpha,\\beta)$ is the proposal distribution. Finally the exercise asks for the log weights which are computed by:\n",
    "\n",
    "$$\n",
    "\\log w_s = \\log p(y_i\\mid\\alpha_s,\\beta_s,n_i,x_i) +\\log p(\\alpha_s,\\beta_s) -\\log p(\\alpha_s,\\beta_s); \\alpha_s,\\beta_s \\sim p(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\log w_s = \\log p(y_i\\mid\\alpha_s,\\beta_s,n_i,x_i); \\alpha_s,\\beta_s \\sim p(\\alpha,\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d28003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0</li><li>10</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td> 4</td><td> 12</td></tr>\n",
       "\t<tr><td>12</td><td>100</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 2 of type dbl\n",
       "\\begin{tabular}{ll}\n",
       "\t  4 &  12\\\\\n",
       "\t 12 & 100\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 2 of type dbl\n",
       "\n",
       "|  4 |  12 |\n",
       "| 12 | 100 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2]\n",
       "[1,]  4    12 \n",
       "[2,] 12   100 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Proposal distribution is a bivariate Gaussian with parameters\n",
    "mean   = c(0,10)\n",
    "cov_xy = 0.6 * 10 * 2\n",
    "\n",
    "cov    = array(c( 2**2, cov_xy , cov_xy, 10**2 ), dim = c(2,2) )\n",
    "\n",
    "mean\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee18137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Likelihood\n",
    "data(\"bioassay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400ee1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 4 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>x</th><th scope=col>n</th><th scope=col>y</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>-0.86</td><td>5</td><td>0</td></tr>\n",
       "\t<tr><td>-0.30</td><td>5</td><td>1</td></tr>\n",
       "\t<tr><td>-0.05</td><td>5</td><td>3</td></tr>\n",
       "\t<tr><td> 0.73</td><td>5</td><td>5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 4 × 3\n",
       "\\begin{tabular}{lll}\n",
       " x & n & y\\\\\n",
       " <dbl> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t -0.86 & 5 & 0\\\\\n",
       "\t -0.30 & 5 & 1\\\\\n",
       "\t -0.05 & 5 & 3\\\\\n",
       "\t  0.73 & 5 & 5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 4 × 3\n",
       "\n",
       "| x &lt;dbl&gt; | n &lt;int&gt; | y &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| -0.86 | 5 | 0 |\n",
       "| -0.30 | 5 | 1 |\n",
       "| -0.05 | 5 | 3 |\n",
       "|  0.73 | 5 | 5 |\n",
       "\n"
      ],
      "text/plain": [
       "  x     n y\n",
       "1 -0.86 5 0\n",
       "2 -0.30 5 1\n",
       "3 -0.05 5 3\n",
       "4  0.73 5 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-0.86</li><li>-0.3</li><li>-0.05</li><li>0.73</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -0.86\n",
       "\\item -0.3\n",
       "\\item -0.05\n",
       "\\item 0.73\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -0.86\n",
       "2. -0.3\n",
       "3. -0.05\n",
       "4. 0.73\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] -0.86 -0.30 -0.05  0.73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0</li><li>1</li><li>3</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "3. 3\n",
       "4. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 1 3 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>5</li><li>5</li><li>5</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5\n",
       "2. 5\n",
       "3. 5\n",
       "4. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5 5 5 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bioassay\n",
    "\n",
    "x = bioassay$x\n",
    "y = bioassay$y\n",
    "n = bioassay$n\n",
    "\n",
    "x\n",
    "y\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f90b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "invlogit <- function(t){\n",
    "    t = exp(t)\n",
    "    t/(1+t)\n",
    "}\n",
    "\n",
    "log_importance_weights <- function( alpha, beta ){\n",
    "    \n",
    "    ## parameter of the likelihood\n",
    "    teta_i = invlogit(alpha + outer(beta , x)) ## these are the parameters for each data point i and each sample \n",
    "                                               #  from the proposal. Each col corresponds to a data point i and each\n",
    "                                               #  row to a parameter\n",
    "    \n",
    "    ## create some usefull intermediate memory copies for faster computation\n",
    "    y <- t(replicate(length(beta), y))\n",
    "    n <- t(replicate(length(beta), n))\n",
    "        \n",
    "    ## compute log likelihood per point in the matrix\n",
    "    #llh = dbinom(y,n,teta_i, log = TRUE) # for a reason with dbinom it does not work properly\n",
    "    llh = y*log(teta_i) + (n-y)*log(1-teta_i)\n",
    "    \n",
    "    ## we now sum since the product in the likelihood is converted into a sum. We sum the rows which are the \n",
    "    #  likelihoods of each datapoint under the proposal\n",
    "    rowSums(llh)\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807de48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-8.95</li><li>-23.47</li><li>-6.02</li><li>-8.13</li><li>-16.61</li><li>-14.57</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -8.95\n",
       "\\item -23.47\n",
       "\\item -6.02\n",
       "\\item -8.13\n",
       "\\item -16.61\n",
       "\\item -14.57\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -8.95\n",
       "2. -23.47\n",
       "3. -6.02\n",
       "4. -8.13\n",
       "5. -16.61\n",
       "6. -14.57\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  -8.95 -23.47  -6.02  -8.13 -16.61 -14.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_test <- c(1.896, -3.6, 0.374, 0.964, -3.123, -1.581)\n",
    "beta_test  <- c(24.76, 20.04, 6.15, 18.65, 8.16, 17.4)\n",
    "round(log_importance_weights( alpha_test, beta_test ),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffc35a",
   "metadata": {},
   "source": [
    "## d)\n",
    "\n",
    "This part of the exercise asks for the normalized importance weights. The log weights where given by:\n",
    "\n",
    "$$\n",
    "w_s = \\log p(y_i\\mid\\alpha_s,\\beta_s,n_i,x_i); \\alpha_s,\\beta_s \\sim p(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "for each sample. Normalized weights are computed by:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{w_i}{\\sum^S_i w_i} \n",
    "$$\n",
    "\n",
    "This means that the log normalized weight is given by:\n",
    "\n",
    "$$\n",
    "\\log w_i = \\log w_i - \\log \\sum^S_i w_i = \\\\\n",
    "%\n",
    "%\n",
    "\\log w_i - \\log \\sum^S_i \\exp \\log w_i \n",
    "$$\n",
    "\n",
    "which means we could compute the normalized weight directly from the log of the unormalized weight in a numerical stable way using logsumexp trick. In this way only one $\\exp$ is needed. Finally the normalized weight is given by:\n",
    "\n",
    "$$\n",
    "w_i =\\exp(\\log w_i - \\log \\sum^S_i \\exp \\log w_i) \n",
    "$$\n",
    "\n",
    "However the exercise asks first to exponentiate and then normalize, or at least that is what I have understood. However later in part e) I have realized that if I dont use logSumExp then things saturate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3888270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_importance_weights <- function( alpha, beta ){\n",
    "    \n",
    "#     ws     <- log_importance_weights(alpha,beta)\n",
    "#     ws_sum <- logSumExp(ws)\n",
    "#     ws     <- exp( ws - ws_sum)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be14f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_importance_weights <- function( alpha, beta ){\n",
    "    \n",
    "    ws     <- log_importance_weights(alpha,beta)\n",
    "    ws     <- exp(ws) # this also gives some weights to NaN\n",
    "    ws_sum <- sum(ws, na.rm = TRUE) # since the above gives NaN the overall sum would be NaN as well.\n",
    "    ws / ws_sum\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f760fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.045</li><li>0</li><li>0.852</li><li>0.103</li><li>0</li><li>0</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.045\n",
       "\\item 0\n",
       "\\item 0.852\n",
       "\\item 0.103\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.045\n",
       "2. 0\n",
       "3. 0.852\n",
       "4. 0.103\n",
       "5. 0\n",
       "6. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.045 0.000 0.852 0.103 0.000 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_test <- c(1.896, -3.6, 0.374, 0.964, -3.123, -1.581)\n",
    "beta_test  <- c(24.76, 20.04, 6.15, 18.65, 8.16, 17.4)\n",
    "round(normalized_importance_weights(alpha = alpha_test, beta = beta_test),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714c972",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33252c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples <- rmvnorm(4000, mean, cov)\n",
    "alpha   <- samples[,1]\n",
    "beta    <- samples[,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cf37ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4000"
      ],
      "text/latex": [
       "4000"
      ],
      "text/markdown": [
       "4000"
      ],
      "text/plain": [
       "[1] 4000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ws <- normalized_importance_weights( alpha, beta )\n",
    "length(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a124702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC/VBMVEUAAAABAQECAgIDAwME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\nSIAAIQEChAQIEBIg8P83auWsfzSsUgAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title “Histogram of ws”"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(ws, breaks = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51006a16",
   "metadata": {},
   "source": [
    "## f)\n",
    "\n",
    "The effective sample size is given by:\n",
    "\n",
    "$$\n",
    "S_\\text{eff} = \\frac{1}{\\sum_s w^{(s)^2}}\n",
    "$$\n",
    "\n",
    "provided the weights are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eaa5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_eff <- function(alpha, beta){\n",
    "    \n",
    "    ws    <- normalized_importance_weights(alpha,beta)\n",
    "    S_eff <- 1 / sum( ws**2, na.rm = TRUE )\n",
    "    S_eff\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4b58ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1161.842"
      ],
      "text/latex": [
       "1161.842"
      ],
      "text/markdown": [
       "1161.842"
      ],
      "text/plain": [
       "[1] 1161.842"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_test <- c(1.896, -3.6, 0.374, 0.964, -3.123, -1.581)\n",
    "beta_test  <- c(24.76, 20.04, 6.15, 18.65, 8.16, 17.4)\n",
    "round(S_eff(alpha = alpha, beta = beta),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908617c",
   "metadata": {},
   "source": [
    "## g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bd4de",
   "metadata": {},
   "source": [
    "The effective sample size represents the equivalent number of samples from a direct Monte Carlo if all the samples from the sampler (in this case the importance sampler) are used. In other words using the $4000$ samples we have drawn is equivalent to using 419.4 from a direct Monte Carlo.\n",
    "\n",
    "\n",
    "Note that since we have drawn $4000$ samples. If all the samples are perfect (a ratio of 1) then this means that the perfect normalized importance weights would have a value of $\\frac{1}{4000}=0.00025$. We can see in the histogram that there are very few normalized importance weights around $0.00025$ which explains this low value of $S_\\text{eff}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd6719",
   "metadata": {},
   "source": [
    "## h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335b1f6",
   "metadata": {},
   "source": [
    "This exercise asks for the posterior mean using a self normalized importance sampler. For our particular exercise we have the following. First we have the posterior:\n",
    "\n",
    "$$\n",
    "p(\\theta | x,y,n) = c \\prod_i p(y_i\\mid\\alpha,\\beta,n_i,x_i)  p(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "%\n",
    "\\mathbb{E}[\\theta] = c \\int \\theta \\, \\hat{p}(\\theta | x,y,n) \\text{d}\\theta = \\\\\n",
    "%\n",
    "%\n",
    "\\frac{c \\int \\theta(\\alpha,\\beta)\\, \\hat{p}(\\alpha,\\beta | x,y,n) \\frac{p(\\alpha,\\beta)}{p(\\alpha,\\beta)}\\text{d}\\theta}{c \\int  \\hat{p}(\\alpha,\\beta | x,y,n)  \\frac{p(\\alpha,\\beta)}{p(\\alpha,\\beta)}} \\approx \\\\ \n",
    "%\n",
    "%\n",
    "\\frac{\\sum_s \\theta(\\alpha_s,\\beta_s) w_s }{\\sum_s w_s};\\alpha_s,\\beta_s \\sim p(\\alpha,\\beta) \n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\log w_s = \\log p(y_i\\mid\\alpha_s,\\beta_s,n_i,x_i)\n",
    "$$\n",
    "\n",
    "Okay, after writting this I have realized the exercise refers to the expected value of $\\alpha$ and $\\beta$. This simplifies everything to:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_s [\\alpha_s,\\beta_s] w_s }{\\sum_s w_s};\\alpha_s,\\beta_s \\sim p(\\alpha,\\beta) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b18295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean <- function(alpha, beta){\n",
    "    \n",
    "    ## get log importance weights\n",
    "    ws_log <- log_importance_weights( alpha, beta ) \n",
    "    \n",
    "    ## denominator (would be better with log sum exp)\n",
    "    den  <- sum(exp(ws_log), na.rm = TRUE )\n",
    "    \n",
    "    ## numerator\n",
    "    alpha_mean <- sum(alpha * exp(ws_log), na.rm = TRUE ) \n",
    "    beta_mean  <- sum(beta  * exp(ws_log), na.rm = TRUE ) \n",
    "    \n",
    "    ## posterior means\n",
    "    c( alpha_mean / den, beta_mean / den )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99103631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.941</li><li>10.538</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.941\n",
       "\\item 10.538\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.941\n",
       "2. 10.538\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0.941 10.538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_test <- c(1.896, -3.6, 0.374, 0.964, -3.123, -1.581)\n",
    "beta_test  <- c(24.76, 20.04, 6.15, 18.65, 8.16, 17.4)\n",
    "round(posterior_mean(alpha = alpha, beta = beta),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ee990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assignment set:\n",
      "assignment4: Bayesian Data Analysis: Assignment 4\n",
      "The assignment contain the following (4) tasks:\n",
      "- log_importance_weights\n",
      "- normalized_importance_weights\n",
      "- S_eff\n",
      "- posterior_mean\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m✔\u001b[39m | \u001b[33mF\u001b[39m \u001b[35mW\u001b[39m \u001b[34mS\u001b[39m \u001b[32m OK\u001b[39m | Context\n",
      "\r",
      "⠏ |         0 | task-1-subtask-1-tests                                                                                                                                                 \r",
      "⠏ |         0 | log_importance_weights()                                                                                                                                               \r",
      "⠋ |         1 | log_importance_weights()                                                                                                                                               \r",
      "\u001b[32m✔\u001b[39m |         5 | log_importance_weights()\u001b[90m [0.2s]\u001b[39m\n",
      "\r",
      "⠏ |         0 | task-2-subtask-1-tests                                                                                                                                                 \r",
      "⠏ |         0 | normalized_importance_weights()                                                                                                                                        \r",
      "\u001b[32m✔\u001b[39m |         5 | normalized_importance_weights()\n",
      "\r",
      "⠏ |         0 | task-3-subtask-1-tests                                                                                                                                                 \r",
      "⠏ |         0 | S_eff()                                                                                                                                                                \r",
      "\u001b[32m✔\u001b[39m |         5 | S_eff()\n",
      "\r",
      "⠏ |         0 | task-4-subtask-1-tests                                                                                                                                                 \r",
      "⠏ |         0 | posterior_mean()                                                                                                                                                       \r",
      "\u001b[32m✔\u001b[39m |         5 | posterior_mean()\n",
      "\n",
      "══ \u001b[1mResults\u001b[22m ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\u001b[36mDuration: 0.2 s\u001b[39m\n",
      "\n",
      "[ FAIL 0 | WARN 0 | SKIP 0 | \u001b[32mPASS\u001b[39m 20 ]\n",
      "Good work!"
     ]
    }
   ],
   "source": [
    "library(markmyassignment)\n",
    "assignment_path <-\n",
    "paste(\"https://github.com/avehtari/BDA_course_Aalto/\",\n",
    "\"blob/master/assignments/tests/assignment4.yml\", sep=\"\")\n",
    "set_assignment(assignment_path)\n",
    "# To check your code/functions, just run\n",
    "mark_my_assignment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
